{"cells":[{"cell_type":"markdown","source":["# SQL Sat MN  Lab#2 - Code notebooks"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"14f5ff75-e03f-481f-b4e9-6a774eddb5aa"},{"cell_type":"code","source":["# If notebook run previously - drop the existing table for reload\n","spark.sql(\"DROP TABLE IF EXISTS ppp_loan_details\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4bc972a9-b04a-4bff-8b7a-a0b7dfd9fa9e"},{"cell_type":"code","source":["# Load file path listing to a dictionary for the next step. We're only using half the files for time\n","file_dict_1 = {\n","        1 : \"Files/sqlsat-labs/PPP/public_up_to_150k_1_230630.csv\",\n","        2 : \"Files/sqlsat-labs/PPP/public_up_to_150k_2_230630.csv\",\n","        3 : \"Files/sqlsat-labs/PPP/public_up_to_150k_3_230630.csv\",\n","        4 : \"Files/sqlsat-labs/PPP/public_up_to_150k_4_230630.csv\",\n","        5 : \"Files/sqlsat-labs/PPP/public_up_to_150k_5_230630.csv\",\n","        6 : \"Files/sqlsat-labs/PPP/public_up_to_150k_6_230630.csv\",\n","        7 : \"Files/sqlsat-labs/PPP/public_up_to_150k_7_230630.csv\",\n","        8 : \"Files/sqlsat-labs/PPP/public_150k_plus_230630.csv\"\n","}\n","print(file_dict_1)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fb9bdf39-0d1a-4706-a90a-87d03d2b39e3"},{"cell_type":"code","source":["from pyspark.sql.types import LongType, DecimalType, DateType\n","from pyspark.sql.functions import lit, col, concat\n","\n","# Init var\n","first_file = True\n","\n","# Loop through our file dictionary. Read each file, load to df, select reduced column list, write to delta table\n","for key, v in file_dict_1.items():\n","    print(f\"Key: {key}, Value: {v}\")\n","\n","    # Note that the file paths are treated differently if you're in DE persona vs. PBI persona\n","    # df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(\"abfss://Demos@onelake.dfs.fabric.microsoft.com/PPP.Lakehouse/\"+v)\n","    df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(v) \n","\n","    # Change the table write mode\n","    if first_file == True :\n","        mode = \"overwrite\"\n","        first_file = False #changing the flag to false for next run\n","        df.printSchema()\n","    else:\n","        mode = \"append\"\n","\n","    df = df.withColumn(\"LoanNumber\", df.LoanNumber) \\\n","        .withColumn(\"DateApproved\", df.DateApproved) \\\n","        .withColumn(\"BorrowerName\", df.BorrowerName) \\\n","        .withColumn(\"BorrowerCity\", df.BorrowerCity) \\\n","        .withColumn(\"BorrowerState\", df.BorrowerState) \\\n","        .withColumn(\"BorrowerZipCode\", df.BorrowerZip) \\\n","        .withColumn(\"ApprovedAmount\", df.CurrentApprovalAmount.cast(DecimalType(10,2))) \\\n","        .withColumn(\"FranchiseName\", df.FranchiseName) \\\n","        .withColumn(\"LenderName\", df.ServicingLenderName) \\\n","        .withColumn(\"RuralorUrban\", df.RuralUrbanIndicator) \\\n","        .withColumn(\"BusinessAge\", df.BusinessAgeDescription) \\\n","        .withColumn(\"EmployeeCount\", df.JobsReported.cast(LongType())) \\\n","        .withColumn(\"NAICSIndustryCode\", df.NAICSCode) \\\n","        .withColumn(\"BusinessTypeDesc\", df.BusinessType)\n","    \n","    # Select a smaller subset of the columns\n","    # df_selection = df.select(\"LoanNumber\",\"DateApproved\",\"BorrowerName\",\"BorrowerCity\",\"BorrowerState\",\"BorrowerZipCode\",\"ApprovedAmount\",\"FranchiseName\",\"LenderName\",\"RuralorUrban\",\"BusinessAge\",\"EmployeeCount\",\"NAICSIndustryCode\",\"BusinessTypeDesc\")\n","    df_selection = df.select(\"LoanNumber\",\"DateApproved\",\"BorrowerName\",\"BorrowerCity\",\"BorrowerState\",\"BorrowerZipCode\",\"ApprovedAmount\",\"FranchiseName\",\"LenderName\",\"RuralorUrban\",\"BusinessAge\",\"EmployeeCount\",\"BusinessTypeDesc\")\n","\n","    # display(df)\n","    display(df_selection)\n","\n","    print(f'Writing {key} data to table - {df.count()} records')  \n","    # Write to delta table\n","    df_selection.write.mode(mode).format('delta').save(f\"Tables/ppp_loan_details\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6649026e-f302-4d10-b24c-dc48fafce103"},{"cell_type":"code","source":["# Read delta table back into a spark df and print a row count\n","df_table = spark.read.table(\"ppp_loan_details\")\n","print(df_table.count())"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6b56534a-a4b7-4175-ae18-23a1fb04e6fd"},{"cell_type":"code","source":["# Display the spark df\n","display(df_table)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21465ae7-72aa-43b0-a4db-150ec562d6cf"},{"cell_type":"code","source":["%%sql\n","--Switch to SQL and query a count\n","SELECT COUNT(*) FROM ppp_loan_details"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"9eeb795c-b23d-4716-abf4-8105a67facd8"},{"cell_type":"code","source":["%%sql\n","--Query sum\n","SELECT \n","    BorrowerState, \n","    SUM(ApprovedAmount) AS TotalApprovedAmount \n","FROM ppp_loan_details\n","GROUP BY BorrowerState\n","ORDER BY TotalApprovedAmount DESC"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"a83ce854-c4e9-4f5f-9ecc-ebf7fab43ea4"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"e671a371-210b-4a6d-87ce-a8e9f2031eb9"}],"default_lakehouse":"e671a371-210b-4a6d-87ce-a8e9f2031eb9","default_lakehouse_name":"DuckLake","default_lakehouse_workspace_id":"32aa0522-2d2c-41ae-b11d-db4faf2f14f0"}}},"nbformat":4,"nbformat_minor":5}